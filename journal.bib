@article{eris2014comparative,
  title={A comparative analysis of multimodal communication during design sketching in co-located and distributed environments},
  author={Eris, Ozgur and Martelaro, Nikolas and Badke-Schaub, Petra},
  journal={Design Studies},
  volume={35},
  number={6},
  pages={559--592},
  year={2014},
  publisher={Elsevier}
}

@article{wu2021learning-INAGT,
author = {Wu, Tong and Martelaro, Nikolas and Stent, Simon and Ortiz, Jorge and Ju, Wendy},
title = {Learning When Agents Can Talk to Drivers Using the INAGT Dataset and Multisensor Fusion},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
abstract = {This paper examines sensor fusion techniques for modeling opportunities for proactive speech-based in-car interfaces. We leverage the Is Now a Good Time (INAGT) dataset, which consists of automotive, physiological, and visual data collected from drivers who self-annotated responses to the question "Is now a good time?," indicating the opportunity to receive non-driving information during a 50-minute drive. We augment this original driver-annotated data with third-party annotations of perceived safety, in order to explore potential driver overconfidence. We show that fusing automotive, physiological, and visual data allows us to predict driver labels of availability, achieving an 0.874 F1-score by extracting statistically relevant features and training with our proposed deep neural network, PazNet. Using the same data and network, we achieve an 0.891 F1-score for predicting third-party labeled safe moments. We train these models to avoid false positives---determinations that it is a good time to interrupt when it is not---since false positives may cause driver distraction or service deactivation by the driver. Our analyses show that conservative models still leave many moments for interaction and show that most inopportune moments are short. This work lays a foundation for using sensor fusion models to predict when proactive speech systems should engage with drivers.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {9},
articleno = {133},
numpages = {28},
keywords = {dataset, multi-modal learning, deep convolutional network, vehicle, interaction timing}
}