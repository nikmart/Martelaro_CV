@article{eris2014comparative,
  title={A comparative analysis of multimodal communication during design sketching in co-located and distributed environments},
  author={Eris, Ozgur and Martelaro, Nikolas and Badke-Schaub, Petra},
  journal={Design Studies},
  volume={35},
  number={6},
  pages={559--592},
  year={2014},
  publisher={Elsevier}
}

@article{wu2021learning-INAGT,
author = {Wu, Tong and Martelaro, Nikolas and Stent, Simon and Ortiz, Jorge and Ju, Wendy},
title = {Learning When Agents Can Talk to Drivers Using the INAGT Dataset and Multisensor Fusion},
year = {2021},
issue_date = {Sept 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
abstract = {This paper examines sensor fusion techniques for modeling opportunities for proactive speech-based in-car interfaces. We leverage the Is Now a Good Time (INAGT) dataset, which consists of automotive, physiological, and visual data collected from drivers who self-annotated responses to the question "Is now a good time?," indicating the opportunity to receive non-driving information during a 50-minute drive. We augment this original driver-annotated data with third-party annotations of perceived safety, in order to explore potential driver overconfidence. We show that fusing automotive, physiological, and visual data allows us to predict driver labels of availability, achieving an 0.874 F1-score by extracting statistically relevant features and training with our proposed deep neural network, PazNet. Using the same data and network, we achieve an 0.891 F1-score for predicting third-party labeled safe moments. We train these models to avoid false positives---determinations that it is a good time to interrupt when it is not---since false positives may cause driver distraction or service deactivation by the driver. Our analyses show that conservative models still leave many moments for interaction and show that most inopportune moments are short. This work lays a foundation for using sensor fusion models to predict when proactive speech systems should engage with drivers.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {9},
articleno = {133},
numpages = {28},
keywords = {dataset, multi-modal learning, deep convolutional network, vehicle, interaction timing}
}



@Article{mti7050053,
AUTHOR = {Weinberg, David and Dwyer, Healy and Fox, Sarah E. and Martelaro, Nikolas},
TITLE = {Sharing the Sidewalk: Observing Delivery Robot Interactions with Pedestrians during a Pilot in Pittsburgh, PA},
JOURNAL = {Multimodal Technologies and Interaction},
VOLUME = {7},
YEAR = {2023},
NUMBER = {5},
ARTICLE-NUMBER = {53},
URL = {https://www.mdpi.com/2414-4088/7/5/53},
ISSN = {2414-4088},
ABSTRACT = {Sidewalk delivery robots are being deployed as a form of last-mile delivery. While many such robots have been deployed on college campuses, fewer have been piloted on public sidewalks. Furthermore, there have been few observational studies of robots and their interactions with pedestrians. To better understand how sidewalk robots might integrate into public spaces, the City of Pittsburgh, Pennsylvania conducted a pilot of sidewalk delivery robots to understand possible uses and the challenges that could arise in interacting with people in the city. Our team conducted ethnographic observations and intercept interviews to understand how residents perceived of and interacted with sidewalk delivery robots over the course of the public pilot. We found that people with limited knowledge about the robots crafted stories about their purpose and function. We observed the robots causing distractions and obstructions with different sidewalk users (including children and dogs), witnessed people helping immobilized robots, and learned about potential accessibility issues that the robots may pose. Based on our findings, we contribute a set of recommendations for future pilots, as well as questions to guide future design for robots in public spaces.},
DOI = {10.3390/mti7050053}
}
