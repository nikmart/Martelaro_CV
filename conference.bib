@inproceedings{eris2010comparative,
  title={A Comparative Analysis of Sketching Interactions of Designers in Co-located and Distributed Environments},
  author={Eris, Ozgur and Martelaro, Nikolas},
  booktitle={Design Thinking Research Symposium - DTRS8},
  pages={149--162},
  year={2010},
  organization={DTRS8}
}

@inproceedings{jung2014participatory,
  title={Participatory materials: having a reflective conversation with an artifact in the making},
  author={Jung, Malte F and Martelaro, Nikolas and Hoster, Halsey and Nass, Clifford},
  booktitle={Proceedings of the 2014 conference on Designing Interactive Systems},
  pages={25--34},
  year={2014}
}

@inproceedings{jung2015using,
  title={Using robots to moderate team conflict: the case of repairing violations},
  author={Jung, Malte F and Martelaro, Nikolas and Hinds, Pamela J},
  booktitle={Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction},
  pages={229--236},
  year={2015}
}


@inproceedings{martelaro2015using,
  title={Using robots to moderate team conflict: The case of repairing violations},
  author={Martelaro, Nikolas and Jung, Malte and Hinds, Pamela},
  booktitle={Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction Extended Abstracts},
  pages={271--271},
  year={2015}
}

@inproceedings{baltodano2015rrads,
  title={The RRADS platform: a real road autonomous driving simulator},
  author={Baltodano, Sonia and Sibi, Srinath and Martelaro, Nikolas and Gowda, Nikhil and Ju, Wendy},
  booktitle={Proceedings of the 7th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
  pages={281--288},
  year={2015}
}

@inproceedings{spadafora2016designing,
  title={Designing the behavior of interactive objects},
  author={Spadafora, Marco and Chahuneau, Victor and Martelaro, Nikolas and Sirkin, David and Ju, Wendy},
  booktitle={Proceedings of the TEI'16: Tenth International Conference on Tangible, Embedded, and Embodied Interaction},
  pages={70--77},
  year={2016},
  organization={ACM}
}

@inproceedings{martelaro2016tell,
  title={Tell Me More: Designing HRI to encourage more trust, disclosure, and companionship},
  author={Martelaro, Nikolas and Nneji, Victoria C and Ju, Wendy and Hinds, Pamela},
  booktitle={HRI '16},
  year={2016}
}

@inproceedings{martelaro2017woz,
  title={WoZ Way: Enabling real-time remote interaction prototyping \& observation in on-road vehicles},
  author={Martelaro, Nikolas and Ju, Wendy},
  booktitle={Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
  pages={169--182},
  year={2017}
}

@inproceedings{moore2017making,
  title={Making noise intentional: A study of servo sound perception},
  author={Moore, Dylan and Martelaro, Nikolas and Ju, Wendy and Tennent, Hamish},
  booktitle={2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI},
  pages={12--21},
  year={2017},
  organization={IEEE}
}

@inproceedings{sirkin2017toward,
  title={Toward measurement of situation awareness in autonomous vehicles},
  author={Sirkin, David and Martelaro, Nikolas and Johns, Mishel and Ju, Wendy},
  booktitle={Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
  pages={405--415},
  year={2017}
}

@inproceedings{martelaro2019exploration,
  title={An Exploration of Speech-Based Productivity Support in the Car},
  author={Martelaro, Nikolas and Teevan, Jaime and Iqbal, Shamsi T},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2019}
}

@inproceedings{semmens2019now,
  title={Is now a good time? an empirical study of vehicle-driver communication timing},
  author={Semmens, Rob and Martelaro, Nikolas and Kaveti, Pushyami and Stent, Simon and Ju, Wendy},
  booktitle={Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
  pages={1--12},
  year={2019}
}

@inproceedings{martelaro2016designing,
  title={Designing HRI to Encourage More Trust, Disclosure, and Companionship, The Eleventh ACM},
  author={Martelaro, Nikolas and Nneji, Victoria C and Ju, Wendy and Hinds, Pamela},
  booktitle={IEEE International Conference on Human Robot Interaction},
  year={2016}
}

@inproceedings{martelaro2020using,
  title={Using Remote Controlled Speech Agents to Explore Music Experience in Context},
  author={Martelaro, Nikolas and Mennicken, Sarah and Thom, Jennifer and Cramer, Henriette and Ju, Wendy},
  booktitle={Proceedings of the 2020 ACM Designing Interactive Systems Conference},
  pages={2065--2076},
  year={2020}
}

@inproceedings{lin2021personal-style,
author = {Lin, David Chuan-En and Martelaro, Nikolas},
title = {Learning Personal Style from Few Examples},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = { A key task in design work is grasping the client’s implicit tastes. Designers often do this based on a set of examples from the client. However, recognizing a common pattern among many intertwining variables such as color, texture, and layout and synthesizing them into a composite preference can be challenging. In this paper, we leverage the pattern recognition capability of computational models to aid in this task. We offer a set of principles for computationally learning personal style. The principles are manifested in PseudoClient, a deep learning framework that learns a computational model for personal graphic design style from only a handful of examples. In several experiments, we found that PseudoClient achieves a 79.40\% accuracy with only five positive and negative examples, outperforming several alternative methods. Finally, we discuss how PseudoClient can be utilized as a building block to support the development of future design applications.},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1566–1578},
numpages = {13},
keywords = {machine learning, graphic design, style, personal preference},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3461778.3462097,
author = {Martelaro, Nikolas and Lakdawala, Tarannum and Chen, Jingya and Hammer, Jessica},
title = {Leveraging the Twitch Platform and Gamification to Generate Home Audio Datasets},
year = {2021},
isbn = {9781450384766},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = { Training AI systems requires large datasets. While there are a range of existing methods for collecting such data, such as paid work on crowdsourcing platforms, the strengths and weaknesses of each method leads us to believe that new, complementary methods are needed. The Polyphonic project contributes a novel method for collecting real-world data by piggybacking on game streaming communities such as Twitch, which capture over a trillion minutes of viewer attention a year. By embedding activities within the sociotechnical context of the stream, we can leverage some of this attention for data collection and processing. In this paper, we describe the design and implementation of a proof-of-concept system for collecting home audio data. We conducted a field study in four live streams and found that our proof-of-concept effectively supports data capture. We also contribute further design insights about stream-based data collection systems.},
booktitle = {Designing Interactive Systems Conference 2021},
pages = {1765–1782},
numpages = {18},
keywords = {datasets, crowdwork, data collection, live streaming, gamification},
location = {Virtual Event, USA},
series = {DIS '21}
}

@inproceedings{10.1145/3434074.3446909,
author = {Zamfirescu-Pereira, J.D. and Sirkin, David and Goedicke, David and LC, Ray and Friedman, Natalie and Mandel, Ilan and Martelaro, Nikolas and Ju, Wendy},
title = {Fake It to Make It: Exploratory Prototyping in HRI},
year = {2021},
isbn = {9781450382908},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Exploratory prototyping techniques are critical to devising new robot forms, actions, and behaviors, and to eliciting human responses to designed interactive features, early in the design process. In this opinion piece, we establish the contribution of exploratory prototyping to the field of human-robot interaction, arguing research engaged in design exploration-rather than controlled experimentation-should be focused on flexibility rather than specificity, possibility rather than replicability, and design insights as incubated subjectively through the designer rather than dispassionately proven by statistical analysis. We draw on literature in HCI for examples of published design explorations in academic venues, and to suggest how analogous contributions can be valued and evaluated by the HRI community. Lastly, we present and examine case studies of three design methods we have used in our own design work: physical prototyping with human-in-the-loop control, video prototyping, and virtual simulations.},
booktitle = {Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {19–28},
numpages = {10},
keywords = {virtual simulation, hri, hci, video prototyping, design, prototyping, experimentation, exploratory prototyping, evaluation, wizard of oz},
location = {Boulder, CO, USA},
series = {HRI '21 Companion}
}

@inproceedings{martelaro2022inclusive,
author = {Martelaro, Nikolas and Carrington, Patrick and Fox, Sarah and Forlizzi, Jodi},
title = {Designing an Inclusive Mobile App for People with Disabilities to Independently Use Autonomous Vehicles},
year = {2022},
isbn = {9781450394154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3543174.3546850},
doi = {10.1145/3543174.3546850},
abstract = {Autonomous vehicles (AVs) could potentially provide independent mobility to people with physical and sensory disabilities. It is critical to understand the needs of people with disabilities and design interfaces specifically for accessible interaction before, during, and while ending an AV trip. Using a research-through-design approach, we identify needs and explore design concepts for a smartphone application to allow people with disabilities to use AVs independently. Through 20 individual interviews, 12 monthly meetings with a group of people with disabilities and transportation advocates, and design prototyping, we develop design concepts for accessible smartphone control of an AV trip. Our work contributes a set of interaction needs for accessible AV interaction, design concepts centered on confidence and control developed in partnership with people with disabilities, and a discussion of an accessible co-design process that other designers can learn from to create accessible automotive user interfaces.},
booktitle = {Proceedings of the 14th International Conference on Automotive User Interfaces and Interactive Vehicular Applications},
pages = {45–55},
numpages = {11},
keywords = {accessibility, participatory design, autonomous vehicles, mobile interfaces},
location = {Seoul, Republic of Korea},
series = {AutomotiveUI '22}
}

@inproceedings{10.1145/3544548.3580999,
author = {Gmeiner, Frederic and Yang, Humphrey and Yao, Lining and Holstein, Kenneth and Martelaro, Nikolas},
title = {Exploring Challenges and Opportunities to Support Designers in Learning to Co-Create with AI-Based Manufacturing Design Tools},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580999},
doi = {10.1145/3544548.3580999},
abstract = {AI-based design tools are proliferating in professional software to assist engineering and industrial designers in complex manufacturing and design tasks. These tools take on more agentic roles than traditional computer-aided design tools and are often portrayed as “co-creators.” Yet, working effectively with such systems requires different skills than working with complex CAD tools alone. To date, we know little about how engineering designers learn to work with AI-based design tools. In this study, we observed trained designers as they learned to work with two AI-based tools on a realistic design task. We find that designers face many challenges in learning to effectively co-create with current systems, including challenges in understanding and adjusting AI outputs and in communicating their design goals. Based on our findings, we highlight several design opportunities to better support designer-AI co-creation.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {226},
numpages = {20},
keywords = {computational co-creation, generative AI, group cognition, team learning, think-aloud study, human-AI collaboration},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{lin2023soundify,
author = {Lin, David Chuan-En and Germanidis, Anastasis and Valenzuela, Crist\'{o}bal and Shi, Yining and Martelaro, Nikolas},
title = {Soundify: Matching Sound Effects to Video},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606823},
doi = {10.1145/3586183.3606823},
abstract = {In the art of video editing, sound helps add character to an object and immerse the viewer within a space. Through formative interviews with professional editors (N=10), we found that the task of adding sounds to video can be challenging. This paper presents Soundify, a system that assists editors in matching sounds to video. Given a video, Soundify identifies matching sounds, synchronizes the sounds to the video, and dynamically adjusts panning and volume to create spatial audio. In a human evaluation study (N=889), we show that Soundify is capable of matching sounds to video out-of-the-box for a diverse range of audio categories. In a within-subjects expert study (N=12), we demonstrate the usefulness of Soundify in helping video editors match sounds to video with lighter workload, reduced task completion time, and improved usability.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {18},
numpages = {13},
keywords = {video, sound effects, foley, audio},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{feffer2023aiincidentdb,
author = {Feffer, Michael and Martelaro, Nikolas and Heidari, Hoda},
title = {The AI Incident Database as an Educational Tool to Raise Awareness of AI Harms: A Classroom Exploration of Efficacy, Limitations, \& Future Improvements},
year = {2023},
isbn = {9798400703812},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3617694.3623223},
doi = {10.1145/3617694.3623223},
abstract = {Prior work has established the importance of integrating AI ethics topics into computer and data sciences curricula. We provide evidence suggesting that one of the critical objectives of AI Ethics education must be to raise awareness of AI harms. While there are various sources to learn about such harms, The AI Incident Database (AIID) is one of the few attempts at offering a relatively comprehensive database indexing prior instances of harms or near harms stemming from the deployment of AI technologies in the real world. This study assesses the effectiveness of AIID as an educational tool to raise awareness regarding the prevalence and severity of AI harms in socially high-stakes domains. We present findings obtained through a classroom study conducted at an R1 institution as part of a course focused on the societal and ethical considerations around AI and ML. Our qualitative findings characterize students’ initial perceptions of core topics in AI ethics and their desire to close the educational gap between their technical skills and their ability to think systematically about ethical and societal aspects of their work. We find that interacting with the database helps students better understand the magnitude and severity of AI harms and instills in them a sense of urgency around (a) designing functional and safe AI and (b) strengthening governance and accountability mechanisms. Finally, we compile students’ feedback about the tool and our class activity into actionable recommendations for the database development team and the broader community to improve awareness of AI harms in AI ethics education.},
booktitle = {Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization},
articleno = {3},
numpages = {11},
keywords = {educational tool, AI safety, classroom exploration, incident database, AI harms},
location = {Boston, MA, USA},
series = {EAAMO '23}
}

@inproceedings{widder2024powerandplay,
author = {Widder, David and Dabbish, Laura and Herbsleb, James and Martelaro, Nikolas},
title = {Power and Play: Investigating ``License to Critique''' in Teams' AI Ethics Discussions},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://arxiv.org/abs/2403.19049},
booktitle = {To appear at the 27th ACM Conference on Computer-Supported Cooperative Work and Social Computing},
location = {San Jos{\'e}, Costa Rica},
series = {CSCW '24}
}

@inproceedings{lin2024jigsaw,
author = {Lin, David Chuan-En and Martelaro, Nikolas},
title = {Jigsaw: Supporting Designers to Prototype Multimodal Applications by Chaining AI Foundation Models},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3641920},
doi = {10.1145/3613904.3641920},
abstract = {Recent advancements in AI foundation models have made it possible for them to be utilized off-the-shelf for creative tasks, including ideating design concepts or generating visual prototypes. However, integrating these models into the creative process can be challenging as they often exist as standalone applications tailored to specific tasks. To address this challenge, we introduce Jigsaw, a prototype system that employs puzzle pieces as metaphors to represent foundation models. Jigsaw allows designers to combine different foundation model capabilities across various modalities by assembling compatible puzzle pieces. To inform the design of Jigsaw, we interviewed ten designers and distilled design goals. In a user study, we showed that Jigsaw enhanced designers’ understanding of available foundation model capabilities, provided guidance on combining capabilities across different modalities and tasks, and served as a canvas to support design exploration, prototyping, and documentation.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {4},
numpages = {15},
keywords = {foundation models, machine learning, multimodal, prototyping, visual programming interface},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{lin2024videogenic,
author = {Lin, David Chuan-En and Caba Heilbron, Fabian and Lee, Joon-Young and Wang, Oliver and Martelaro, Nikolas},
title = {Videogenic: Identifying Highlight Moments in Videos with Professional Photographs as a Prior},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656186},
doi = {10.1145/3635636.3656186},
abstract = {This paper investigates the challenge of extracting highlight moments from videos. To perform this task, we need to understand what constitutes a highlight for arbitrary video domains while at the same time being able to scale across different domains. Our key insight is that photographs taken by photographers tend to capture the most remarkable or photogenic moments of an activity. Drawing on this insight, we present Videogenic, a technique capable of creating domain-specific highlight videos for a diverse range of domains. In a human evaluation study (N=50), we show that a high-quality photograph collection combined with CLIP-based retrieval (which uses a neural network with semantic knowledge of images) can serve as an excellent prior for finding video highlights. In a within-subjects expert study (N=12), we demonstrate the usefulness of Videogenic in helping video editors create highlight videos with lighter workload, shorter task completion time, and better usability.},
booktitle = {Proceedings of the 16th Conference on Creativity \& Cognition},
pages = {328–346},
numpages = {19},
keywords = {video highlight detection, video moment retrieval, video summarization},
location = {Chicago, IL, USA},
series = {C\&C '24}
}

@inproceedings{lin2024videomap,
author = {Lin, David Chuan-En and Caba Heilbron, Fabian and Lee, Joon-Young and Wang, Oliver and Martelaro, Nikolas},
title = {VideoMap: Supporting Video Exploration, Brainstorming, and Prototyping in the Latent Space},
year = {2024},
isbn = {9798400704857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3635636.3656192},
doi = {10.1145/3635636.3656192},
abstract = {Video editing is a creative and complex endeavor and we believe that there is potential for reimagining a new video editing interface to better support the creative and exploratory nature of video editing. We take inspiration from latent space exploration tools that help users find patterns and connections within complex datasets. We present VideoMap, a proof-of-concept video editing interface that operates on video frames projected onto a latent space. We support intuitive navigation through map-inspired navigational elements and facilitate transitioning between different latent spaces through swappable lenses. We built three VideoMap components to support editors in three common video tasks. In a user study with both professionals and non-professionals, editors found that VideoMap helps reduce grunt work, offers a user-friendly experience, provides an inspirational way of editing, and effectively supports the exploratory nature of video editing. We further demonstrate the versatility of VideoMap by implementing three extended applications. For interactive examples, we invite you to visit our project page: https://chuanenlin.com/videomap.},
booktitle = {Proceedings of the 16th Conference on Creativity \& Cognition},
pages = {311–327},
numpages = {17},
keywords = {latent space visualization, video editing interface},
location = {Chicago, IL, USA},
series = {C\&C '24}
}

@inproceedings{akridge2024bus,
author = {Akridge, Hunter and Fan, Bonnie and Tang, Alice Xiaodi and Mehta, Chinar and Martelaro, Nikolas and Fox, Sarah E},
title = {“The bus is nothing without us”: Making Visible the Labor of Bus Operators amid the Ongoing Push Towards Transit Automation},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642714},
doi = {10.1145/3613904.3642714},
abstract = {This paper describes how the circumstances bus operators manage presents unique challenges to the feasibility of high-level automation in public transit. Avoiding an overly rationalized view of bus operators’ labor is critical to ensure the introduction of automation technologies does not compromise public wellbeing, the dignity of transit workers, or the integrity of critical public infrastructure. Our findings from a group interview study show that bus operators take on work — undervalued by those advancing automation technologies — to ensure the well-being of passengers and communities. Notably, bus operators are positioned to function as shock absorbers during social crises in their communities and in moments of technological breakdown as new systems come on board. These roles present a critical argument against the rapid push toward driverless automation in public transit. We conclude by identifying opportunities for participatory design and collaborative human-machine teaming for a more just future of transit.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {479},
numpages = {16},
keywords = {autonomous vehicle (AV) technology, bus operators, critical HCI, human-machine teaming, invisible work, political economy, transit automation},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{han2024codesignrobot,
author = {Han, Howard Ziyu and Li, Franklin Mingzhe and Baca Vazquez, Alesandra and Byrne, Daragh and Martelaro, Nikolas and Fox, Sarah E},
title = {Co-design Accessible Public Robots: Insights from People with Mobility Disability, Robotic Practitioners and Their Collaborations},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642875},
doi = {10.1145/3613904.3642875},
abstract = {Sidewalk robots are increasingly common across the globe. Yet, their operation on public paths poses challenges for people with mobility disabilities (PwMD) who face barriers to accessibility, such as insufficient curb cuts. We interviewed 15 PwMD to understand how they perceive sidewalk robots. Findings indicated that PwMD feel they have to compete for space on the sidewalk when robots are introduced. We next interviewed eight robotics practitioners to learn about their attitudes towards accessibility. Practitioners described how issues often stem from robotic companies addressing accessibility only after problems arise. Both interview groups underscored the importance of integrating accessibility from the outset. Building on this finding, we held four co-design workshops with PwMD and practitioners in pairs. These convenings brought to bear accessibility needs around robots operating in public spaces and in the public interest. Our study aims to set the stage for a more inclusive future around public service robots.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {592},
numpages = {20},
keywords = {Accessibility, Delivery robots, Human-robot interaction, Public space, Sidewalk robots},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{gmeiner2025intenttagging,
author = {Gmeiner, Frederic and Marquardt, Nicolai and Bentley, Michael and Romat, Hugo and Pahud, Michel and Brown, David and Roseway, Asta and Martelaro, Nikolas and Holstein, Kenneth and Hinckley, Ken and Riche, Nathalie},
title = {Intent Tagging: Exploring Micro-Prompting Interactions for Supporting Granular Human-GenAI Co-Creation Workflows},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713861},
doi = {10.1145/3706598.3713861},
abstract = {Despite Generative AI (GenAI) systems’ potential for enhancing content creation, users often struggle to effectively integrate GenAI into their creative workflows. Core challenges include misalignment of AI-generated content with user intentions (intent elicitation and alignment), user uncertainty around how to best communicate their intents to the AI system (prompt formulation), and insufficient flexibility of AI systems to support diverse creative workflows (workflow flexibility). Motivated by these challenges, we created IntentTagger: a system for slide creation based on the notion of Intent Tags—small, atomic conceptual units that encapsulate user intent—for exploring granular and non-linear micro-prompting interactions for Human-GenAI co-creation workflows. Our user study with 12 participants provides insights into the value of flexibly expressing intent across varying levels of ambiguity, meta-intent elicitation, and the benefits and challenges of intent tag-driven workflows. We conclude by discussing the broader implications of our findings and design considerations for GenAI-supported content creation workflows.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {531},
numpages = {31},
keywords = {intent tagging, human-AI interaction, human-AI co-creation, generative AI, rich content creation},
location = {
},
series = {CHI '25}
}

@inproceedings{lin2025inkspire,
author = {Lin, David Chuan-En and Kang, Hyeonsu B. and Martelaro, Nikolas and Kittur, Aniket and Chen, Yan-Ying and Hong, Matthew K.},
title = {Inkspire: Supporting Design Exploration with Generative AI through Analogical Sketching},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713397},
doi = {10.1145/3706598.3713397},
abstract = {With recent advancements in the capabilities of Text-to-Image (T2I) AI models, product designers have begun experimenting with them in their work. However, T2I models struggle to interpret abstract language and the current user experience of T2I tools can induce design fixation rather than a more iterative, exploratory process. To address these challenges, we developed Inkspire, a sketch-driven tool that supports designers in prototyping product design concepts with analogical inspirations and a complete sketch-to-design-to-sketch feedback loop. To inform the design of Inkspire, we conducted an exchange session with designers and distilled design goals for improving T2I interactions. In a within-subjects study comparing Inkspire to ControlNet, we found that Inkspire supported designers with more inspiration and exploration of design ideas, and improved aspects of the co-creative process by allowing designers to effectively grasp the current state of the AI to guide it towards novel design intentions.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {427},
numpages = {18},
keywords = {generative AI, sketching, iterative design, co-creative design},
location = {
},
series = {CHI '25}
}

@inproceedings{kang2025biospark,
author = {Kang, Hyeonsu B and Lin, David Chuan-En and Chen, Yan-Ying and Hong, Matthew K. and Martelaro, Nikolas and Kittur, Aniket},
title = {BioSpark: Beyond Analogical Inspiration to LLM-augmented Transfer},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714053},
doi = {10.1145/3706598.3714053},
abstract = {We present BioSpark, a system for analogical innovation designed to act as a creativity partner in reducing the cognitive effort in finding, mapping, and creatively adapting diverse inspirations. While prior approaches have focused on initial stages of finding inspirations, BioSpark uses LLMs embedded in a familiar, visual, Pinterest-like interface to go beyond inspiration to supporting users in identifying the key solution mechanisms, transferring them to the problem domain, considering tradeoffs, and elaborating on details and characteristics. To accomplish this BioSpark introduces several novel contributions, including a tree-of-life enabled approach for generating relevant and diverse inspirations, as well as AI-powered cards including ‘Sparks’ for analogical transfer; ‘Trade-offs’ for considering pros and cons; and ‘Q&A’ for deeper elaboration. We evaluated BioSpark through workshops with professional designers and a controlled user study, finding that using BioSpark led to a greater number of generated ideas; those ideas being rated higher in creative quality; and more diversity in terms of biological inspirations used than a control condition. Our results suggest new avenues for creativity support tools embedding AI in familiar interaction paradigms for designer workflows.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {653},
numpages = {29},
keywords = {Goal-driven Analogies, Analogical Transfer, Design Creativity, Ideation, Large Language Models},
location = {
},
series = {CHI '25}
}

@inproceedings{yang2024compact,
author = {Yang, Humphrey and Shen, I-Chao and Martelaro, Nikolas and Zhu, Bo and Xie, Haoran and Igarashi, Takeo and Yao, Lining},
title = {CompAct: Designing Interconnected Compliant Mechanisms with Targeted Actuation Transmissions},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3714307},
doi = {10.1145/3706598.3714307},
abstract = {Compliant mechanisms enable the creation of compact and easy-to-fabricate devices for tangible interaction. This work explores interconnected compliant mechanisms consisting of multiple joints and rigid bodies to transmit and process displacements as signals that result from physical interactions. As these devices are difficult to design due to their vast and complex design space, we developed a graph-based design algorithm and computational tool to help users program and customize such computational functions and procedurally model physical designs. When combined with active materials with actuation and sensing capabilities, these devices can also render and detect haptic interaction. Our design examples demonstrate the tool's capability to respond to relevant HCI concepts, including building modular physical interface toolkits, encrypting tangible interactions, and customizing user augmentation for accessibility. We believe the tool will facilitate the generation of new interfaces with enriched affordance.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {1},
numpages = {19},
keywords = {Tangible interface, compliant mechanism, design tool, shape-changing interface},
location = {
},
series = {CHI '25}
}


